{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earthquake Prediction using Swarm AI and MARL\n",
    "\n",
    "This notebook implements a comprehensive solution for earthquake prediction using:\n",
    "- Multi-Agent Reinforcement Learning (MARL)\n",
    "- Swarm Intelligence\n",
    "- Deep Learning\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple\n",
    "import gym\n",
    "from pettingzoo import ParallelEnv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Seismic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_sample_seismic_data(num_records=1000):\n",
    "    # Generate timestamps\n",
    "    base_date = datetime(2024, 1, 1)\n",
    "    timestamps = [base_date + timedelta(hours=i) for i in range(num_records)]\n",
    "    \n",
    "    # Generate sample data\n",
    "    data = {\n",
    "        'time': timestamps,\n",
    "        'latitude': np.random.uniform(25.0, 45.0, num_records),\n",
    "        'longitude': np.random.uniform(120.0, 150.0, num_records),\n",
    "        'depth': np.random.uniform(0, 100, num_records),\n",
    "        'magnitude': np.random.normal(3.0, 1.0, num_records)\n",
    "    }\n",
    "    \n",
    "    # Create some significant earthquakes\n",
    "    significant_indices = np.random.choice(num_records, size=int(num_records * 0.1))\n",
    "    data['magnitude'][significant_indices] = np.random.normal(5.0, 0.5, len(significant_indices))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.sort_values('time')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate and display sample data\n",
    "seismic_df = generate_sample_seismic_data()\n",
    "print(\"Sample seismic data:\")\n",
    "seismic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Seismic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_seismic_data(df):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Magnitude distribution\n",
    "    sns.histplot(data=df, x='magnitude', bins=30, ax=axes[0,0])\n",
    "    axes[0,0].set_title('Magnitude Distribution')\n",
    "    \n",
    "    # Depth distribution\n",
    "    sns.histplot(data=df, x='depth', bins=30, ax=axes[0,1])\n",
    "    axes[0,1].set_title('Depth Distribution')\n",
    "    \n",
    "    # Geographic distribution\n",
    "    sns.scatterplot(data=df, x='longitude', y='latitude', \n",
    "                    hue='magnitude', size='magnitude',\n",
    "                    ax=axes[1,0])\n",
    "    axes[1,0].set_title('Geographic Distribution')\n",
    "    \n",
    "    # Magnitude over time\n",
    "    sns.scatterplot(data=df, x='time', y='magnitude',\n",
    "                    alpha=0.5, ax=axes[1,1])\n",
    "    axes[1,1].set_title('Magnitude over Time')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_seismic_data(seismic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class EarthquakeEnv(ParallelEnv):\n",
    "    def __init__(self, num_agents: int, grid_size: int = 50):\n",
    "        super().__init__()\n",
    "        self.num_agents = num_agents\n",
    "        self.grid_size = grid_size\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        self.action_spaces = {\n",
    "            f\"agent_{i}\": gym.spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "            for i in range(num_agents)\n",
    "        }\n",
    "        \n",
    "        self.observation_spaces = {\n",
    "            f\"agent_{i}\": gym.spaces.Dict({\n",
    "                \"position\": gym.spaces.Box(low=0, high=grid_size, shape=(2,), dtype=np.float32),\n",
    "                \"seismic_features\": gym.spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32),\n",
    "                \"neighbor_data\": gym.spaces.Box(low=-np.inf, high=np.inf, shape=(8,), dtype=np.float32)\n",
    "            }) for i in range(num_agents)\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        self.agents = [f\"agent_{i}\" for i in range(self.num_agents)]\n",
    "        self.agent_positions = {\n",
    "            agent: np.random.uniform(0, self.grid_size, size=(2,))\n",
    "            for agent in self.agents\n",
    "        }\n",
    "        return self._get_observations()\n",
    "\n",
    "    def step(self, actions):\n",
    "        for agent_id, action in actions.items():\n",
    "            self.agent_positions[agent_id] += action\n",
    "            self.agent_positions[agent_id] = np.clip(\n",
    "                self.agent_positions[agent_id], 0, self.grid_size\n",
    "            )\n",
    "        \n",
    "        observations = self._get_observations()\n",
    "        rewards = self._compute_rewards(actions)\n",
    "        dones = {agent: False for agent in self.agents}\n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "        \n",
    "        return observations, rewards, dones, infos\n",
    "\n",
    "    def _get_observations(self):\n",
    "        observations = {}\n",
    "        for agent in self.agents:\n",
    "            observations[agent] = {\n",
    "                \"position\": self.agent_positions[agent],\n",
    "                \"seismic_features\": self._get_seismic_features(agent),\n",
    "                \"neighbor_data\": self._get_neighbor_data(agent)\n",
    "            }\n",
    "        return observations\n",
    "\n",
    "    def _get_seismic_features(self, agent):\n",
    "        return np.random.normal(0, 1, size=(4,))\n",
    "\n",
    "    def _get_neighbor_data(self, agent):\n",
    "        current_pos = self.agent_positions[agent]\n",
    "        neighbor_data = []\n",
    "        for other_agent in self.agents:\n",
    "            if other_agent != agent:\n",
    "                other_pos = self.agent_positions[other_agent]\n",
    "                distance = np.linalg.norm(current_pos - other_pos)\n",
    "                neighbor_data.extend([distance, *other_pos])\n",
    "        return np.array(neighbor_data[:8])\n",
    "\n",
    "    def _compute_rewards(self, actions):\n",
    "        return {agent: np.random.uniform(-1, 1) for agent in self.agents}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swarm Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SwarmAgent(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.obs_encoder = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "        \n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.velocity = torch.zeros(action_dim)\n",
    "        self.best_position = None\n",
    "        self.best_fitness = float('-inf')\n",
    "    \n",
    "    def forward(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        encoded = self.obs_encoder(obs)\n",
    "        action_logits = self.policy_head(encoded)\n",
    "        value = self.value_head(encoded)\n",
    "        return action_logits, value\n",
    "    \n",
    "    def update_position_pso(self, global_best, w=0.7, c1=1.5, c2=1.5):\n",
    "        r1, r2 = torch.rand(1), torch.rand(1)\n",
    "        self.velocity = (w * self.velocity + \n",
    "                        c1 * r1 * (self.best_position - self.get_position()) +\n",
    "                        c2 * r2 * (global_best - self.get_position()))\n",
    "        new_position = self.get_position() + self.velocity\n",
    "        self.set_position(new_position)\n",
    "    \n",
    "    def get_position(self):\n",
    "        return torch.cat([p.data.flatten() for p in self.parameters()])\n",
    "    \n",
    "    def set_position(self, position):\n",
    "        start = 0\n",
    "        for param in self.parameters():\n",
    "            num_params = param.numel()\n",
    "            param.data = position[start:start + num_params].reshape(param.shape)\n",
    "            start += num_params\n",
    "\n",
    "class SwarmNetwork:\n",
    "    def __init__(self, num_agents: int, obs_dim: int, action_dim: int):\n",
    "        self.agents = [SwarmAgent(obs_dim, action_dim) for _ in range(num_agents)]\n",
    "        self.global_best_position = None\n",
    "        self.global_best_fitness = float('-inf')\n",
    "    \n",
    "    def update_swarm(self, fitness_scores: List[float]):\n",
    "        for agent, fitness in zip(self.agents, fitness_scores):\n",
    "            if fitness > agent.best_fitness:\n",
    "                agent.best_fitness = fitness\n",
    "                agent.best_position = agent.get_position()\n",
    "                \n",
    "                if fitness > self.global_best_fitness:\n",
    "                    self.global_best_fitness = fitness\n",
    "                    self.global_best_position = agent.get_position()\n",
    "        \n",
    "        for agent in self.agents:\n",
    "            agent.update_position_pso(self.global_best_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 num_agents: int,\n",
    "                 obs_dim: int,\n",
    "                 action_dim: int,\n",
    "                 learning_rate: float = 3e-4):\n",
    "        self.env = EarthquakeEnv(num_agents)\n",
    "        self.swarm_network = SwarmNetwork(num_agents, obs_dim, action_dim)\n",
    "        self.optimizers = [\n",
    "            Adam(agent.parameters(), lr=learning_rate)\n",
    "            for agent in self.swarm_network.agents\n",
    "        ]\n",
    "        \n",
    "        self.training_history = {\n",
    "            'episode_rewards': [],\n",
    "            'average_fitness': []\n",
    "        }\n",
    "    \n",
    "    def train_episode(self, max_steps: int = 1000) -> Tuple[float, List[float]]:\n",
    "        observations = self.env.reset()\n",
    "        episode_rewards = []\n",
    "        swarm_fitness_scores = [0] * len(self.swarm_network.agents)\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            actions = {}\n",
    "            for idx, agent in enumerate(self.swarm_network.agents):\n",
    "                obs = torch.FloatTensor(self._process_observation(\n",
    "                    observations[f\"agent_{idx}\"]))\n",
    "                action_logits, value = agent(obs)\n",
    "                action = torch.tanh(action_logits).detach().numpy()\n",
    "                actions[f\"agent_{idx}\"] = action\n",
    "            \n",
    "            new_observations, rewards, dones, _ = self.env.step(actions)\n",
    "            \n",
    "            for idx, reward in rewards.items():\n",
    "                agent_idx = int(idx.split('_')[1])\n",
    "                swarm_fitness_scores[agent_idx] += reward\n",
    "            \n",
    "            episode_rewards.append(sum(rewards.values()))\n",
    "            observations = new_observations\n",
    "            \n",
    "            if all(dones.values()):\n",
    "                break\n",
    "        \n",
    "        self.swarm_network.update_swarm(swarm_fitness_scores)\n",
    "        \n",
    "        # Update training history\n",
    "        self.training_history['episode_rewards'].append(sum(episode_rewards))\n",
    "        self.training_history['average_fitness'].append(np.mean(swarm_fitness_scores))\n",
    "        \n",
    "        return sum(episode_rewards), swarm_fitness_scores\n",
    "    \n",
    "    def _process_observation(self, obs: dict) -> np.ndarray:\n",
    "        return np.concatenate([\n",
    "            obs['position'],\n",
    "            obs['seismic_features'],\n",
    "            obs['neighbor_data']\n",
    "        ])\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "        \n",
    "        # Plot episode rewards\n",
    "        ax1.plot(self.training_history['episode_rewards'])\n",
    "        ax1.set_title('Episode Rewards over Time')\n",
    "        ax1.set_xlabel('Episode')\n",
    "        ax1.set_ylabel('Total Reward')\n",
    "        \n",
    "        # Plot average fitness\n",
    "        ax2.plot(self.training_history['average_fitness'])\n",
    "        ax2.set_title('Average Agent Fitness over Time')\n",
    "        ax2.set_xlabel('Episode')\n",
    "        ax2.set_ylabel('Average Fitness')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training parameters\n",
    "num_agents = 10\n",
    "obs_dim = 14  # position(2) + seismic_features(4) + neighbor_data(8)\n",
    "action_dim = 2\n",
    "num_episodes = 100  # Reduced for demonstration\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(num_agents, obs_dim, action_dim)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    total_reward, fitness_scores = trainer.train_episode()\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}\")\n",
    "        print(f\"Total Reward: {total_reward:.2f}\")\n",
    "        print(f\"Average Agent Fitness: {np.mean(fitness_scores):.2f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Plot training results\n",
    "trainer.plot_training_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_model(trainer, num_evaluation_episodes=10):\n",
    "    evaluation_rewards = []\n",
    "    evaluation_fitness = []\n",
    "    \n",
    "    for episode in range(num_evaluation_episodes):\n",
    "        total_reward, fitness_scores = trainer.train_episode()\n",
    "        evaluation_rewards.append(total_reward)\n",
    "        evaluation_fitness.append(np.mean(fitness_scores))\n",
    "    \n",
    "    print(\"\\nModel Evaluation Results:\")\n",
    "    print(f\"Average Episode Reward: {np.mean(evaluation_rewards):.2f} ± {np.std(evaluation_rewards):.2f}\")\n",
    "    print(f\"Average Agent Fitness: {np.mean(evaluation_fitness):.2f} ± {np.std(evaluation_fitness):.2f}\")\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_model(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Agent Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def visualize_agent_behavior(trainer, steps=50):\n",
    "    observations = trainer.env.reset()\n",
    "    \n    positions_history = []\n",
    "    \n    for step in range(steps):\n",
    "        actions = {}\n",
    "        current_positions = {}\n",
    "        \n        for idx, agent in enumerate(trainer.swarm_network.agents):\n",
    "            obs = torch.FloatTensor(trainer._process_observation(\n",
    "                observations[f\"agent_{idx}\"]))\n",
    "            action_logits, _ = agent(obs)\n",
    "            action = torch.tanh(action_logits).detach().numpy()\n",
    "            actions[f\"agent_{idx}\"] = action\n",
    "            current_positions[f\"agent_{idx}\"] = observations[f\"agent_{idx}\"][\"position\"]\n",
    "        \n        positions_history.append(current_positions)\n",
    "        observations, _, _, _ = trainer.env.step(actions)\n",
    "    \n    # Visualize agent movements\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(trainer.swarm_network.agents)))\n",
    "    \n    for idx, agent_id in enumerate(trainer.env.agents):\n",
    "        positions = np.array([pos[agent_id] for pos in positions_history])\n",
    "        plt.plot(positions[:, 0], positions[:, 1], 'o-', \n",
    "                 color=colors[idx], label=f'Agent {idx}',\n",
    "                 alpha=0.5, markersize=3)\n",
    "    \n    plt.title('Agent Movement Patterns')\n",
    "    plt.xlabel('X Position')\n",
    "    plt.ylabel('Y Position')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize agent behavior\n",
    "visualize_agent_behavior(trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}